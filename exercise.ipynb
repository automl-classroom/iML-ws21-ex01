{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrance Test\n",
    "\n",
    "In the following tasks, you are guided to solve a multiclassification task to determine the `type` of wheat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('lightning')\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "pl.utilities.distributed.log.setLevel(logging.ERROR)\n",
    "pl.utilities.seed.log.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Preparing Dataset\n",
    "\n",
    "Before a model can be trained, the dataset has to be loaded, preprocessed and splitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data\n",
    "\n",
    "- Read in the csv file `wheat_seeds.csv`, which consists of 199 data instances with 8 features each. \n",
    "- Divide the data into X and y:\n",
    "  - X should be a two-dimensional numpy array with the shape (199,7) derived from all columns except the last.\n",
    "  - y should be a two-dimensional numpy array with the shape (199,) derived from the last column. This last column contains the `type` value, which we want to determine for unseen\n",
    "    data in the end.\n",
    "- Shuffle the data using `sklearn.utils.shuffle`. Set `random_state=0`.\n",
    "\n",
    "Hints:\n",
    "- Use pandas to read in the file easily and convert them to a numpy array.\n",
    "- Use `shape` to check the dimension of a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data = pd.read_csv(\"wheat_seeds.csv\").to_numpy()\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "X, y = data[:, 0:7], data[:, 7]\n",
    "X, y = shuffle(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preprocessing\n",
    "\n",
    "- Take care of NaN/None values in X and y. Replace them simply with zeros here.\n",
    "- Normalization is most of the time necessary to ensure good convergence in a Neural Network.\n",
    "Transform X s.t. the values are between 0 and 1.\n",
    "\n",
    "Hints:\n",
    "- NaN/None values have to be replaced first.\n",
    "- Use [sklearn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) for the normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X[np.isnan(X)] = 0\n",
    "y[np.isnan(y)] = 0\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Splitting\n",
    "\n",
    "- Split X and y into training and validation data.\n",
    "- Use the ratio 6:4 for training and validation data, respectively.\n",
    "\n",
    "Hint: You can use vanilla numpy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X[:int(len(X)*0.6)], y[:int(len(y)*0.6)]\n",
    "X_val, y_val = X[int(len(X)*0.6):], y[int(len(y)*0.6):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Multi Layer Perceptron\n",
    "\n",
    "We now want to use our training and validation data to fit a model. As model we use a Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Defining Sequential Model\n",
    "\n",
    "- To implement the model, we use pytorch lightning. Please have a look into the [introductions](https://pytorch-lightning.readthedocs.io/en/latest/starter/new-project.html) to get a short overview.\n",
    "- Define a [sequential model](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) in the following way:\n",
    "    - The first linear layer should take the number of features from X (already given as `input_units`) as `in_features` and `num_hidden_layers` as `out_features`.\n",
    "    - The last linear layer should have `output_units` as `out_features`.\n",
    "    - In between, use `num_hidden_layers` and `num_hidden_units` to set the hidden linear layers and\n",
    "      their features, respectively.\n",
    "    - `num_hidden_layers` tells us how many layers should be used and\n",
    "      `num_hidden_units` how many units in each of these layers are used.\n",
    "    - After every linear layer, a ReLU as activation function follows.\n",
    "    - However, please leave out the activation after the last linear layer.\n",
    "\n",
    "Hint: Use *list to use a list as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequential_model(num_hidden_layers,\n",
    "                         num_hidden_units,\n",
    "                         input_units,\n",
    "                         output_units):\n",
    "    \"\"\"\n",
    "    Returns a sequential model with 2+num_hidden_layers linear layers.\n",
    "    All linear layers (except the last one) are followed by a ReLU function.\n",
    "\n",
    "    Parameters:\n",
    "        num_hidden_layers (int): The number of hidden layers.\n",
    "        num_hidden_units (int): The number of features from the hidden\n",
    "            linear layers.\n",
    "        input_units (int): The number of input units.\n",
    "            Should be number of features.\n",
    "        output_units (int): The number of output units. In case of regression task,\n",
    "            it should be one.\n",
    "\n",
    "    Returns:\n",
    "        model (nn.Sequential): Neural network as sequential model.\n",
    "    \"\"\"\n",
    "\n",
    "    layers = [nn.Linear(input_units, num_hidden_units), nn.ReLU(),]\n",
    "\n",
    "    for layer in range(num_hidden_layers):\n",
    "        layers += [nn.Linear(num_hidden_units, num_hidden_units), nn.ReLU(),]\n",
    "\n",
    "    layers += [nn.Linear(num_hidden_units, output_units)]\n",
    "\n",
    "    seq_layers = nn.Sequential(*layers)\n",
    "\n",
    "    return seq_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Since we have numpy data, it is required to convert\n",
    "    them into PyTorch tensors first.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        \n",
    "        self.y = torch.zeros((self.X.shape[0], 1), dtype=torch.float32)\n",
    "        if y is not None:\n",
    "            y = y.astype(np.float32)\n",
    "            self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Multi Layer Perceptron wrapper for pytorch lightning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sequential_model, verbose=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sequential_model: Underlying Neural Network.\n",
    "            verbose (bool): If true, the highest val accuracy is plotted after every epoch.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        self.highest_val_accuracy = 0\n",
    "\n",
    "        # Important to init the weights the same way\n",
    "        pl.seed_everything(0)\n",
    "        self.model = sequential_model\n",
    "        \n",
    "        # (Multi-)Classification problem\n",
    "        self.loss_fn = nn.CrossEntropyLoss() \n",
    "    \n",
    "    def calculate_accuracy(self, y_pred, y_test):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy for `y_pred` and `y_test`.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred_softmax = torch.log_softmax(y_pred, dim=1)\n",
    "        _, y_pred_tags = torch.max(y_pred_softmax, dim=1)    \n",
    "        \n",
    "        correct_pred = (y_pred_tags == y_test).float()\n",
    "        acc = correct_pred.sum() / len(correct_pred)\n",
    "        \n",
    "        return acc\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        \"\"\"\n",
    "        Receives the validation data and calculates the accuracy on them.\n",
    "\n",
    "        Parameters:\n",
    "            batch: Tuple of validation data.\n",
    "\n",
    "        Returns: \n",
    "            metrics (dict): Dict with val accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = batch\n",
    "        y_hat = self.model(X)\n",
    "        \n",
    "        val_accuracy = self.calculate_accuracy(y_hat, y)\n",
    "        return {'val_accuracy': val_accuracy}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Collects the outputs from `validation_step` and sets\n",
    "        the average for the accuracy.\n",
    "\n",
    "        Parameters:\n",
    "            outputs: List of dicts from `validation_step`.\n",
    "        \"\"\"\n",
    "\n",
    "        val_accuracy = torch.stack([o['val_accuracy'] for o in outputs]).numpy().flatten()\n",
    "        \n",
    "        val_accuracy = float(np.mean(val_accuracy))\n",
    "        if val_accuracy > self.highest_val_accuracy:\n",
    "            self.highest_val_accuracy = val_accuracy\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"{self.current_epoch}: {self.highest_val_accuracy}\")\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        \"\"\"\n",
    "        Receives the training data and calculates\n",
    "        cross entropy as loss, which is used to train\n",
    "        the classifier.\n",
    "\n",
    "        Parameters:\n",
    "            batch: Tuple of training data.\n",
    "\n",
    "        Returns: \n",
    "            loss (Tensor): Loss of current step. \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = batch\n",
    "        y_hat = self.model(X)\n",
    "\n",
    "        return self.loss_fn(y_hat, y.long())\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures Adam as optimizer.\n",
    "\n",
    "        Returns:\n",
    "            optimizer (torch.optim): Optimizer used internally by\n",
    "                pytorch lightning.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def fit(self, X_train, y_train,\n",
    "                  X_val, y_val,\n",
    "                  learning_rate=1e-1,\n",
    "                  num_epochs=10,\n",
    "                  batch_size=8):\n",
    "        \"\"\"\n",
    "        Fits the model with training data. Model is validated after every epoch on the validation\n",
    "        data.\n",
    "        \n",
    "        Parameters:\n",
    "            X_train, y_train: Training data.\n",
    "            X_val, y_val: Validation data.\n",
    "            learning_rate (float): Learning rate used in the optimizer.\n",
    "            num_epochs (int): Number of epochs.\n",
    "            batch_size (int): How many instances are used to update the weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        pl.seed_everything(0)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.trainer = pl.Trainer(\n",
    "            num_sanity_val_steps=0,  # No validation sanity\n",
    "            max_epochs=num_epochs,  # We only train one epoch\n",
    "            progress_bar_refresh_rate=0,  # No progress bar\n",
    "            weights_summary=None  # No model summary\n",
    "        )\n",
    "\n",
    "        # Define training loader\n",
    "        # `train_loader` is a lambda function, which takes batch_size as input\n",
    "        train_loader = DataLoader(\n",
    "            PyTorchDataset(X_train, y_train),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0)\n",
    "\n",
    "        # Define validation loader\n",
    "        val_loader = DataLoader(\n",
    "            PyTorchDataset(X_val, y_val),\n",
    "            batch_size=1,\n",
    "            num_workers=0)\n",
    "\n",
    "        # Train model\n",
    "        self.trainer.fit(self, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training the model\n",
    "\n",
    "Define the input and the output units and pass them to the `get_sequential_model` function.\\\n",
    "The output units should be determined with the help of `np.unique` since we have one neuron per class. The input\n",
    "units are the number of used X features. Choose the other parameters as you please.\n",
    "\n",
    "Afterwards, initialize the `MLP` model with verbose and call its fit method to verify that\n",
    "everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.7250000238418579\n",
      "1: 0.887499988079071\n",
      "2: 0.887499988079071\n",
      "3: 0.887499988079071\n",
      "4: 0.887499988079071\n",
      "5: 0.887499988079071\n",
      "6: 0.887499988079071\n",
      "7: 0.887499988079071\n",
      "8: 0.9125000238418579\n",
      "9: 0.925000011920929\n"
     ]
    }
   ],
   "source": [
    "input_units = X_train.shape[1]\n",
    "all_labels = np.unique(list(y_train) + list(y_val))\n",
    "output_units = len(all_labels)\n",
    "\n",
    "seq_model = get_sequential_model(num_hidden_layers=1, num_hidden_units=5, input_units=input_units, output_units=output_units)\n",
    "\n",
    "model = MLP(seq_model, verbose=True)\n",
    "model.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Random Search\n",
    "\n",
    "In practice, it is often not clear how to choose hyperparameters for a specific problem. Random\n",
    "search can be used to found a suitable configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Writing Random Search\n",
    "\n",
    "- Perform random search to find the best configuration for the given multiclassification problem.\n",
    "- Write a loop that trains a MLP based on the config.\n",
    "- All results should be saved in the dict `results` with config as key and accuracy as value.\n",
    "\n",
    "Hints:\n",
    "- Use model.highest_val_accuracy to judge the performance of a configuration. \n",
    "- Access hyperparameters from config with e.g. `config[\"lr\"]`.\n",
    "- Use all defined hyperparameters.\n",
    "- Execution may take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "from tqdm import tqdm\n",
    "\n",
    "cs = CS.ConfigurationSpace(seed=0)\n",
    "\n",
    "learning_rate_hp = CSH.UniformFloatHyperparameter('learning_rate', lower=1e-5, upper=1e-1)\n",
    "num_hidden_layers = CSH.UniformIntegerHyperparameter('num_hidden_layers', lower=0, upper=4)\n",
    "num_hidden_units = CSH.UniformIntegerHyperparameter('num_hidden_units', lower=1, upper=int(X_train.shape[1] * 2))\n",
    "batch_size_hp = CSH.UniformIntegerHyperparameter('batch_size', lower=8, upper=int(len(X_train)))\n",
    "cs.add_hyperparameters([learning_rate_hp, num_hidden_layers, num_hidden_units, batch_size_hp])\n",
    "\n",
    "results = {}\n",
    "for config in tqdm(cs.sample_configuration(25)):\n",
    "    sequential_model = get_sequential_model(\n",
    "        num_hidden_layers=config[\"num_hidden_layers\"],\n",
    "        num_hidden_units=config[\"num_hidden_units\"],\n",
    "        input_units=input_units,\n",
    "        output_units=output_units)\n",
    "\n",
    "    model = MLP(sequential_model)\n",
    "    model.fit(X_train, y_train, X_val, y_val, batch_size=config[\"batch_size\"], learning_rate=config[\"learning_rate\"])\n",
    "    \n",
    "    results[config] = model.highest_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Plotting\n",
    "\n",
    "- Print the best and the worst configuration with their corresponding performance.\n",
    "- Use matplotlib to plot a step function (use `x` and `y`), showing the highest accuracy found so far over time (in our case: number of evaluated configurations).\n",
    "- Make sure the steps are placed \"post\".\n",
    "\n",
    "Hints:\n",
    "- Use the results dict from the exercise before.\n",
    "- Have a look into the [API](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.step.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best Configuration:\n",
      "  batch_size, Value: 67\n",
      "  learning_rate, Value: 0.061213451315014926\n",
      "  num_hidden_layers, Value: 0\n",
      "  num_hidden_units, Value: 8\n",
      " with 94.9999988079071%.\n",
      "Found worst Configuration:\n",
      "  batch_size, Value: 95\n",
      "  learning_rate, Value: 0.006031944908210691\n",
      "  num_hidden_layers, Value: 4\n",
      "  num_hidden_units, Value: 11\n",
      " with 30.000001192092896%.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "config_list = list(results.keys())\n",
    "accuracy_list = list(results.values())\n",
    "\n",
    "best = np.argmax(np.array(accuracy_list))\n",
    "worst = np.argmin(np.array(accuracy_list))\n",
    "\n",
    "highest_accuracy = accuracy_list[best]\n",
    "lowest_accuracy = accuracy_list[worst]\n",
    "\n",
    "best_config = config_list[best]\n",
    "worst_config = config_list[worst]\n",
    "\n",
    "current_lowest_accuracy = lowest_accuracy\n",
    "for idx, (config, accuracy) in enumerate(results.items()):\n",
    "    x += [idx]\n",
    "    if accuracy > current_lowest_accuracy:\n",
    "        current_lowest_accuracy = accuracy\n",
    "        \n",
    "    y += [current_lowest_accuracy]\n",
    "    \n",
    "plt.step(x, y, where='post')\n",
    "plt.xlabel(\"Configurations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "print(f\"Found best {best_config} with {highest_accuracy*100}%.\")\n",
    "print(f\"Found worst {worst_config} with {lowest_accuracy*100}%.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1342e92b59d0c3e7eb4a40a884088d66588e8a7c2ca0ffba069e80e4ff639c09"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
